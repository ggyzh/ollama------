{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_root_url=\"http://weixiao-mini2.local:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Local Models:\n",
      "Name: llava:latest\n",
      "Modified At: 2024-05-15T12:53:55.974782498+08:00\n",
      "Size: 4733363377 bytes\n",
      "Digest: 8dd30f6b0cb19f555f2c7a7ebda861449ea2cc76bf1f44e262931f45fc81d081\n",
      "Details:\n",
      "  parent_model: \n",
      "  format: gguf\n",
      "  family: llama\n",
      "  families: ['llama', 'clip']\n",
      "  parameter_size: 7B\n",
      "  quantization_level: Q4_0\n",
      "\n",
      "Name: impactframes/ifai_promptmkr_dolphin_phi3:latest\n",
      "Modified At: 2024-05-15T01:17:05.101913514+08:00\n",
      "Size: 4108127903 bytes\n",
      "Digest: ad124488129f1aca966a7a284334365ecf6efc2867a980b1bcc652f1e3b87f6d\n",
      "Details:\n",
      "  parent_model: \n",
      "  format: gguf\n",
      "  family: llama\n",
      "  families: ['llama']\n",
      "  parameter_size: 7B\n",
      "  quantization_level: Q4_1\n",
      "\n",
      "Name: impactframes/llama3_ifai_sd_prompt_mkr_q4km:latest\n",
      "Modified At: 2024-05-15T01:14:59.939215057+08:00\n",
      "Size: 4920748425 bytes\n",
      "Digest: 2ed36d99240581eb9fcd05ae083136de35aeac94c6d03689457e240d24eea63f\n",
      "Details:\n",
      "  parent_model: \n",
      "  format: gguf\n",
      "  family: llama\n",
      "  families: ['llama']\n",
      "  parameter_size: 8B\n",
      "  quantization_level: Q4_K_M\n",
      "\n",
      "Name: brxce/stable-diffusion-prompt-generator:latest\n",
      "Modified At: 2024-05-15T01:09:12.840655585+08:00\n",
      "Size: 4108917578 bytes\n",
      "Digest: 474a09318a2e9e88320ecd6792b1142ac7bb9f78fbf991e1856d75ccaea4378a\n",
      "Details:\n",
      "  parent_model: \n",
      "  format: gguf\n",
      "  family: llama\n",
      "  families: ['llama']\n",
      "  parameter_size: 7B\n",
      "  quantization_level: Q4_0\n",
      "\n",
      "Name: nomic-embed-text:latest\n",
      "Modified At: 2024-05-15T00:12:21.918281737+08:00\n",
      "Size: 274302450 bytes\n",
      "Digest: 0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f\n",
      "Details:\n",
      "  parent_model: \n",
      "  format: gguf\n",
      "  family: nomic-bert\n",
      "  families: ['nomic-bert']\n",
      "  parameter_size: 137M\n",
      "  quantization_level: F16\n",
      "\n",
      "Name: llama3-zh-inst:latest\n",
      "Modified At: 2024-05-14T19:04:06.217361713+08:00\n",
      "Size: 8540771588 bytes\n",
      "Digest: 5951eb47ba6aa177082fc467d053311d773a4e78cfac1d30d7434554ff7e5dd7\n",
      "Details:\n",
      "  parent_model: \n",
      "  format: gguf\n",
      "  family: llama\n",
      "  families: ['llama']\n",
      "  parameter_size: 8.0B\n",
      "  quantization_level: Q8_0\n",
      "\n",
      "Name: llama2:latest\n",
      "Modified At: 2024-05-14T11:55:27.754996786+08:00\n",
      "Size: 3826793677 bytes\n",
      "Digest: 78e26419b4469263f75331927a00a0284ef6544c1975b826b15abdaef17bb962\n",
      "Details:\n",
      "  parent_model: \n",
      "  format: gguf\n",
      "  family: llama\n",
      "  families: ['llama']\n",
      "  parameter_size: 7B\n",
      "  quantization_level: Q4_0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_local_models(server_url):\n",
    "    \"\"\"\n",
    "    获取Ollama服务器上本地可用模型列表。\n",
    "    \n",
    "    :param server_url: Ollama服务器的API端点\n",
    "    :return: 模型列表（如果成功）或错误信息（如果失败）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 发送GET请求到Ollama服务器\n",
    "        response = requests.get(server_url)\n",
    "\n",
    "        # 检查响应状态码\n",
    "        if response.status_code == 200:\n",
    "            # 解析响应内容\n",
    "            models_info = response.json()\n",
    "            models = models_info.get('models', [])\n",
    "            \n",
    "            return models\n",
    "        else:\n",
    "            return f\"Failed to get models from Ollama server: {response.status_code} {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "def print_models(models):\n",
    "    \"\"\"\n",
    "    打印模型列表的详细信息。\n",
    "    \n",
    "    :param models: 模型列表\n",
    "    \"\"\"\n",
    "    print(\"Available Local Models:\")\n",
    "    for model in models:\n",
    "        print(f\"Name: {model['name']}\")\n",
    "        print(f\"Modified At: {model['modified_at']}\")\n",
    "        print(f\"Size: {model['size']} bytes\")\n",
    "        print(f\"Digest: {model['digest']}\")\n",
    "        print(f\"Details:\")\n",
    "        details = model.get('details', {})\n",
    "        for key, value in details.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义Ollama服务器地址\n",
    "    ollama_server_url = ollama_root_url+'/api/tags'\n",
    "    \n",
    "    # 获取本地可用模型列表\n",
    "    result = get_local_models(ollama_server_url)\n",
    "    \n",
    "    # 检查结果并打印模型详细信息\n",
    "    if isinstance(result, list):\n",
    "        print_models(result)\n",
    "    else:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama3-zh-inst:latest\n",
      "Created At: 2024-05-24T16:50:42.696089Z\n",
      "Response: The sky appears blue because of a phenomenon called Rayleigh scattering, which occurs when sunlight interacts with the Earth's atmosphere.\n",
      "\n",
      "When sunlight enters the Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen and oxygen. These molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths. This is because the smaller molecules are more effective at scattering the shorter wavelengths.\n",
      "\n",
      "As a result, when we look up at the sky during the daytime, we see mostly blue light being scattered back towards us from the atmosphere. The red and orange colors are not as easily scattered, so they appear less prominent in the sky. At sunrise and sunset, when the sun is lower in the sky, the light has to travel through more of the Earth's atmosphere before reaching our eyes. This means that even more blue light is scattered away, leaving mainly red and orange wavelengths to reach us, which is why we see these colors during those times.\n",
      "\n",
      "In summary, the sky appears blue because of Rayleigh scattering, where sunlight interacts with tiny molecules in the Earth's atmosphere, scattering shorter (blue) wavelengths more than longer (red) wavelengths.\n",
      "Done: True\n",
      "Context: [128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 118195, 122503, 102264, 35304, 103129, 105390, 103129, 46034, 1811, 128009, 128006, 882, 128007, 271, 10445, 374, 279, 13180, 6437, 30, 128009, 128006, 78191, 128007, 271, 791, 13180, 8111, 6437, 1606, 315, 264, 25885, 2663, 13558, 64069, 72916, 11, 902, 13980, 994, 40120, 84261, 449, 279, 9420, 596, 16975, 382, 4599, 40120, 29933, 279, 9420, 596, 16975, 11, 433, 35006, 13987, 35715, 315, 45612, 1778, 439, 47503, 323, 24463, 13, 4314, 35715, 45577, 279, 3177, 304, 682, 18445, 11, 719, 814, 45577, 24210, 320, 12481, 8, 93959, 810, 1109, 5129, 320, 1171, 8, 93959, 13, 1115, 374, 1606, 279, 9333, 35715, 527, 810, 7524, 520, 72916, 279, 24210, 93959, 382, 2170, 264, 1121, 11, 994, 584, 1427, 709, 520, 279, 13180, 2391, 279, 62182, 11, 584, 1518, 10213, 6437, 3177, 1694, 38067, 1203, 7119, 603, 505, 279, 16975, 13, 578, 2579, 323, 19087, 8146, 527, 539, 439, 6847, 38067, 11, 779, 814, 5101, 2753, 21102, 304, 279, 13180, 13, 2468, 64919, 323, 44084, 11, 994, 279, 7160, 374, 4827, 304, 279, 13180, 11, 279, 3177, 706, 311, 5944, 1555, 810, 315, 279, 9420, 596, 16975, 1603, 19261, 1057, 6548, 13, 1115, 3445, 430, 1524, 810, 6437, 3177, 374, 38067, 3201, 11, 9564, 14918, 2579, 323, 19087, 93959, 311, 5662, 603, 11, 902, 374, 3249, 584, 1518, 1521, 8146, 2391, 1884, 3115, 382, 644, 12399, 11, 279, 13180, 8111, 6437, 1606, 315, 13558, 64069, 72916, 11, 1405, 40120, 84261, 449, 13987, 35715, 304, 279, 9420, 596, 16975, 11, 72916, 24210, 320, 12481, 8, 93959, 810, 1109, 5129, 320, 1171, 8, 93959, 13, 128009]\n",
      "Total Duration: 56556926208\n",
      "Load Duration: 14298677791\n",
      "Prompt Eval Count: 35\n",
      "Prompt Eval Duration: 500326000\n",
      "Eval Count: 236\n",
      "Eval Duration: 41756180000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def generate_response(server_url, model_name, prompt):\n",
    "    \"\"\"\n",
    "    发送请求到Ollama服务器以生成响应。\n",
    "    \n",
    "    :param server_url: Ollama服务器的API端点\n",
    "    :param model_name: 要使用的模型名称\n",
    "    :param prompt: 要发送的提示\n",
    "    :return: 响应结果（如果成功）或错误信息（如果失败）\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 发送POST请求到Ollama服务器\n",
    "        response = requests.post(server_url, json=payload)\n",
    "\n",
    "        # 检查响应状态码\n",
    "        if response.status_code == 200:\n",
    "            # 解析响应内容\n",
    "            result = response.json()\n",
    "            return result\n",
    "        else:\n",
    "            return f\"Failed to generate response from Ollama server: {response.status_code} {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "def print_response(response):\n",
    "    \"\"\"\n",
    "    打印生成的响应的详细信息。\n",
    "    \n",
    "    :param response: 响应结果\n",
    "    \"\"\"\n",
    "    if isinstance(response, dict):\n",
    "        print(\"Model:\", response.get(\"model\"))\n",
    "        print(\"Created At:\", response.get(\"created_at\"))\n",
    "        print(\"Response:\", response.get(\"response\"))\n",
    "        print(\"Done:\", response.get(\"done\"))\n",
    "        print(\"Context:\", response.get(\"context\"))\n",
    "        print(\"Total Duration:\", response.get(\"total_duration\"))\n",
    "        print(\"Load Duration:\", response.get(\"load_duration\"))\n",
    "        print(\"Prompt Eval Count:\", response.get(\"prompt_eval_count\"))\n",
    "        print(\"Prompt Eval Duration:\", response.get(\"prompt_eval_duration\"))\n",
    "        print(\"Eval Count:\", response.get(\"eval_count\"))\n",
    "        print(\"Eval Duration:\", response.get(\"eval_duration\"))\n",
    "    else:\n",
    "        print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "      # 定义Ollama服务器地址\n",
    "    ollama_server_url = ollama_root_url+'/api/generate'\n",
    "    \n",
    "    # 设置模型名称和提示\n",
    "    model_name = \"llama3-zh-inst:latest\"\n",
    "    prompt = \"Why is the sky blue?\"\n",
    "    \n",
    "    # 生成响应\n",
    "    result = generate_response(ollama_server_url, model_name, prompt)\n",
    "    \n",
    "    # 打印响应详细信息\n",
    "    print_response(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llava:latest', 'created_at': '2024-05-24T16:59:57.476569Z', 'response': ' 这是一张婷女卧在床上的照片。她正在向我们摄像头，表情不明，裹子下半身露出。她戴着短裙和绿色牛仔短裸。她有长, dark hair and appears to be in a relaxed pose. The background is simple with a bed frame visible, suggesting this might be taken in a bedroom setting. ', 'done': True, 'done_reason': 'stop', 'context': [733, 16289, 28793, 28705, 29176, 28971, 28969, 30162, 31401, 29675, 28914, 29183, 29369, 28804, 733, 28748, 16289, 28793, 28705, 29176, 28971, 28969, 30162, 232, 172, 186, 29933, 232, 144, 170, 29010, 232, 189, 141, 29054, 28914, 30131, 29369, 28944, 30715, 29302, 29010, 29478, 29242, 29550, 31128, 29663, 29640, 28924, 29024, 29418, 28988, 29381, 28924, 235, 166, 188, 29169, 29061, 30458, 29894, 31771, 29065, 28944, 30715, 233, 139, 183, 30163, 30132, 235, 166, 156, 29131, 234, 190, 194, 29395, 30857, 231, 190, 151, 30132, 235, 166, 187, 28944, 30715, 28998, 29495, 28725, 3199, 3691, 304, 8045, 298, 347, 297, 264, 18788, 15596, 28723, 415, 5414, 349, 3588, 395, 264, 2855, 4108, 9141, 28725, 20223, 456, 1659, 347, 3214, 297, 264, 9384, 5587, 28723, 28705], 'total_duration': 28507134833, 'load_duration': 7959804458, 'prompt_eval_count': 1, 'prompt_eval_duration': 8993044000, 'eval_count': 106, 'eval_duration': 11485207000}\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "# 定义Ollama服务器地址\n",
    "ollama_server_url = ollama_root_url+'/api/generate'\n",
    "\n",
    "# 读取图像并进行 base64 编码\n",
    "with open(\"/Users/yinzhihua/Desktop/example_image.jpg\", \"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# 构建请求的 payload\n",
    "payload = {\n",
    "    \"model\": \"llava:latest\",\n",
    "    \"prompt\": \"这是一张怎样的图片?\",\n",
    "    \"stream\": False,\n",
    "    \"images\": [base64_image]\n",
    "}\n",
    "\n",
    "# 发送POST请求\n",
    "response = requests.post(ollama_server_url, json=payload)\n",
    "\n",
    "# 解析响应\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3-zh-inst:latest', 'created_at': '2024-05-25T05:14:33.45819Z', 'response': 'The reason why we see a predominantly blue color in our skies during daytime has to do with how light interacts and scatters through the Earth’s atmosphere', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 118195, 122503, 102264, 35304, 103129, 105390, 103129, 46034, 1811, 128009, 128006, 882, 128007, 271, 10445, 374, 279, 13180, 6437, 30, 128009, 128006, 78191, 128007, 271, 791, 2944, 3249, 584, 1518, 264, 47904, 6437, 1933, 304, 1057, 50393, 2391, 62182, 706, 311, 656, 449, 1268, 3177, 84261, 323, 1156, 10385, 1555, 279, 9420, 753, 16975, 128009], 'total_duration': 27425482875, 'load_duration': 10438243292, 'prompt_eval_count': 35, 'prompt_eval_duration': 6421801000, 'eval_count': 30, 'eval_duration': 10563788000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def generate(self, model, prompt, options):\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": options\n",
    "        }\n",
    "        response = requests.post(self.url, data=json.dumps(payload), headers={\"Content-Type\": \"application/json\"})\n",
    "        return response.json()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    ollama_root_url=\"http://weixiao-mini2.local:11434\"\n",
    "    # 定义Ollama服务器地址\n",
    "    ollama_server_url = ollama_root_url+'/api/generate'\n",
    "    model = \"llama3-zh-inst:latest\"\n",
    "    prompt = \"Why is the sky blue?\"\n",
    "    options = {\n",
    "        \"num_keep\": 5,\n",
    "        \"seed\": 42,\n",
    "        \"num_predict\": 100,\n",
    "        \"top_k\": 20,\n",
    "        \"top_p\": 0.9,\n",
    "        \"tfs_z\": 0.5,\n",
    "        \"typical_p\": 0.7,\n",
    "        \"repeat_last_n\": 33,\n",
    "        \"temperature\": 0.8,\n",
    "        \"repeat_penalty\": 1.2,\n",
    "        \"presence_penalty\": 1.5,\n",
    "        \"frequency_penalty\": 1.0,\n",
    "        \"mirostat\": 1,\n",
    "        \"mirostat_tau\": 0.8,\n",
    "        \"mirostat_eta\": 0.6,\n",
    "        \"penalize_newline\": True,\n",
    "        \"stop\": [\"\\n\", \"user:\"],\n",
    "        \"numa\": False,\n",
    "        \"num_ctx\": 1024,\n",
    "        \"num_batch\": 2,\n",
    "        \"num_gpu\": 1,\n",
    "        \"main_gpu\": 0,\n",
    "        \"low_vram\": False,\n",
    "        \"f16_kv\": True,\n",
    "        \"vocab_only\": False,\n",
    "        \"use_mmap\": True,\n",
    "        \"use_mlock\": False,\n",
    "        \"num_thread\": 8\n",
    "    }\n",
    "\n",
    "    client = LLMClient(ollama_server_url)\n",
    "    response = client.generate(model, prompt, options)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3-zh-inst:latest', 'created_at': '2024-05-25T05:28:39.458257Z', 'message': {'role': 'assistant', 'content': '瑞利散射和米氏散射是两种不同的光学现象，它们的主要区别在于它们发生的原因不同。\\n瑞利散射是一种由气体分子对短波长（如蓝色）光线进行散射而产生的现象。这种散射是由于气体分子的极小尺寸和相互之间的距离，使得它们能够与短波长的光线发生有效的碰撞，从而使得这些光线被散射到各个方向上。\\n 米氏散射则是一种由大颗粒（如水滴、云雾等）对较长波长（如红色）光线进行散射而产生的现象。这种散射是由于大颗粒与光线之间的相互作用，使得它们能够改变光线的传播方向，从而使得这些光线被散射到各个方向上。\\n 因此，瑞利散射主要发生在短波长（如蓝色）光线下，而米氏散射则主要发生在较长波长（如红色）光线下。两者的区别在于它们的散射原理和发生条件不同。'}, 'done_reason': 'stop', 'done': True, 'total_duration': 48214927291, 'load_duration': 3206375, 'prompt_eval_count': 15, 'prompt_eval_duration': 627477000, 'eval_count': 263, 'eval_duration': 47578472000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "class Message:\n",
    "    def __init__(self, role, content):\n",
    "        self.role = role\n",
    "        self.content = content\n",
    "\n",
    "class OllamaChat:\n",
    "    def __init__(self, root_url=\"http://weixiao-mini2.local:11434\"):\n",
    "        self.root_url = root_url\n",
    "        self.server_url = f\"{self.root_url}/api/chat\"\n",
    "\n",
    "    def send_message(self, model, messages, stream=False):\n",
    "        url = self.server_url\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            response.raise_for_status()\n",
    "\n",
    "# Example usage\n",
    "ollama_chat = OllamaChat()\n",
    "model = \"llama3-zh-inst:latest\"\n",
    "messages = [\n",
    "    Message(\"user\", \"why is the sky blue?\"),\n",
    "    Message(\"assistant\", \"due to rayleigh scattering.\"),\n",
    "    Message(\"user\", \"how is that different than mie scattering?\"),\n",
    "     Message(\"user\", \"请使用中文来回答\")\n",
    "]\n",
    "stream = False\n",
    "response = ollama_chat.send_message(model, [vars(msg) for msg in messages], stream)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
